
#+TITLE: BSC HPC

* Introduction

This document summarises the BSC's HPC environment.  You can find the
official documentation [[https://www.bsc.es/supportkc/][here]].  In this document should be viewed as a
quickstart guide for MareNostrum 5 (MN5). It is by no means exhaustive. 

* System overview

The computational nodes of MN5 belong to two groups: General Purpose
Partition (GPP) and Accelerated Partition (ACC). The latter being
fitted with GPU's. These are separate partitions, meaning the
available environments, (e.g., python modules), may be different.
All nodes have acces to the General Parallel File System (GPFS). 

* Login and transfer nodes

Jobs can be submitted to the computational nodes and data transfered
to the GPFS via login-nodes and transfer-nodes, respectively. The
login-nodes and transfer-nodes are accessible from outside the
BSC. These nodes not access the outside for security reasons.

|---------------+------------------+-----------|
| Node type     | Node             | Partition |
|---------------+------------------+-----------|
| Login         | glogin1.bsc.es.  | GPP       |
|               | glogin2.bsc.es   | GPP       |
|---------------+------------------+-----------|
| Login         | alogin1.bsc.es   | ACC       |
|               | alogin2.bsc.es   | ACC       |
|---------------+------------------+-----------|
| Data transfer | transfer1.bsc.es | shared    |
|               | transfer2.bsc.es | shared    |
|               | transfer3.bsc.es | shared    |
|               | transfer4.bsc.es | shared    |
|---------------+------------------+-----------|
* Login

You can log in into one of the login and transfer nodes using ssh:

#+BEGIN_SRC
mylaptop$> ssh {username}@glogin1.bsc.es
mylaptop$> ssh {username}@alogin1.bsc.es
mylaptop$> ssh {username}@transfer1.bsc.es
#+END_SRC

To avoid being prompted for your password each time, you can setup a passkey, instructions [[https://wiki.archlinux.org/title/SSH_keys][here]].  

* GPFS

The GPFS is a global file system, accessible from both the GPP and ACC
nodes. There is shared storage space for your reserach group under
~/gpfs/project/<GROUP>~ . In the case of ICONBI, the group is
bsc79. There is a storage quota per group that you can check running
~bsc_quota~.

* Computational environment (modules)

available environments:
#+begin_src bash
module avail
#+end_src

load environment:
#+begin_src bash
 module  load <module name>
#+end_src

For instance:
#+begin_src bash
 module load python/3.10.2
#+end_src

* Jobsubmission

